{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "nltk.download('punkt_tab') is used for downloading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "example_string = \"Generate Values for user, book, car, bike and resources\"\n",
    "tokenized_words = word_tokenize(example_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Stop Words\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "example_string = \"Generate Values for user, book, car, bike and resources\"\n",
    "tokenized_words = word_tokenize(example_string)\n",
    "filtered_list = []\n",
    "for word in tokenized_words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove symbols and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gener', 'valu', 'user', 'book', 'car', 'bike', 'resourc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "example_string = \"Generating Values for user, book, car, bike and resources\"\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', example_string)\n",
    "tokenized_words = word_tokenize(clean_text)\n",
    "filtered_list = []\n",
    "for word in tokenized_words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "\n",
    "# print(filtered_list)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_list]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "* nltk.download('wordnet')\n",
    "* nltk.download('averaged_perceptron_tagger')\n",
    "* nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_string = \"Generating geese Values for user, book, car, bike and resources\"\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', example_string)\n",
    "tokenized_words = word_tokenize(clean_text)\n",
    "filtered_list = []\n",
    "for word in tokenized_words:\n",
    "    filtered_list.append(word)\n",
    "lemma = WordNetLemmatizer()\n",
    "for word in filtered_list:\n",
    "    print(word,\":\",lemma.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging\n",
    "\n",
    "Input parameter must be tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Generating', 'VBG'), ('Values', 'NNS'), ('for', 'IN'), ('user', 'NN'), ('book', 'NN'), ('car', 'NN'), ('bike', 'NN'), ('and', 'CC'), ('resources', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "example_string = \"Generating Values for user, book, car, bike and resources\"\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', example_string)\n",
    "tokenized_words = word_tokenize(clean_text)\n",
    "tagged_string = pos_tag(tokenized_words)\n",
    "print(tagged_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking and Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "example_string = \"Generating Values for user, book, car, bike and resources\"\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', example_string)\n",
    "tokenized_words = word_tokenize(clean_text)\n",
    "tagged_string = pos_tag(tokenized_words)\n",
    "\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "tree = chunk_parser.parse(tagged_string)\n",
    "# tree.draw()\n",
    "\n",
    "chink_grammar = \"\"\"\n",
    "Chink: {<.*>+}\n",
    "       }<CC>{\"\"\"\n",
    "chink_parser = nltk.RegexpParser(chink_grammar)\n",
    "chink_tree = chink_parser.parse(tagged_string)\n",
    "chink_tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "* nltk.download('maxent_ne_chunker_tab')\n",
    "* nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "sample_string = \"Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded the company.\"\n",
    "tokenized_words = word_tokenize(sample_string)\n",
    "tagged_string = pos_tag(tokenized_words)\n",
    "tree = nltk.ne_chunk(tagged_string)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "* import nltk\n",
    "* nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "r , sir ?'--' Soon enough for any honest man that goes a passenger .' Ha ! Jon\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "\n",
    "corpus = gutenberg.words('melville-moby_dick.txt')\n",
    "text = Text(corpus)\n",
    "text.concordance([\"honest\", \"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding \n",
    "### Semantic Similarity Search\n",
    "\n",
    "pip install -U sentence-transformers\n",
    "\n",
    "Common Distance Metrics used\n",
    "* Euclidean\n",
    "* Manhattan\n",
    "* Minkowski\n",
    "* ChebyChev\n",
    "* Cosine Similarity\n",
    "* Hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T50363\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\T50363\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\T50363\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: That is a happy person\n",
      "That is a very happy person  -> similarity score =  0.9429151\n",
      "That is a happy dog  -> similarity score =  0.6945774\n",
      "Today is a sunny day  -> similarity score =  0.25687614\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the model we want to use (it'll download itself)\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = [\n",
    "  \"That is a very happy person\",\n",
    "  \"That is a happy dog\",\n",
    "  \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "# vector embeddings created from dataset\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# query vector embedding\n",
    "query_embedding = model.encode(\"That is a happy person\")\n",
    "\n",
    "# define our distance metric\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "# run semantic similarity search\n",
    "print(\"Query: That is a happy person\")\n",
    "for e, s in zip(embeddings, sentences):\n",
    "    print(s, \" -> similarity score = \",\n",
    "         cosine_similarity(e, query_embedding))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
