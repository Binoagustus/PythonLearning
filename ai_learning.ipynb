{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "nltk.download('punkt_tab') is used for downloading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "example_string = \"Generate Values for user, book, car, bike and resources\"\n",
    "tokenized_words = word_tokenize(example_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Stop Words\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "example_string = \"Generate Values for user, book, car, bike and resources\"\n",
    "tokenized_words = word_tokenize(example_string)\n",
    "filtered_list = []\n",
    "for word in tokenized_words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove symbols and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gener', 'valu', 'user', 'book', 'car', 'bike', 'resourc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "example_string = \"Generating Values for user, book, car, bike and resources\"\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', example_string)\n",
    "tokenized_words = word_tokenize(clean_text)\n",
    "filtered_list = []\n",
    "for word in tokenized_words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "\n",
    "# print(filtered_list)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_list]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "* nltk.download('wordnet')\n",
    "* nltk.download('averaged_perceptron_tagger')\n",
    "* nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_string = \"Generating geese Values for user, book, car, bike and resources\"\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', example_string)\n",
    "tokenized_words = word_tokenize(clean_text)\n",
    "filtered_list = []\n",
    "for word in tokenized_words:\n",
    "    filtered_list.append(word)\n",
    "lemma = WordNetLemmatizer()\n",
    "for word in filtered_list:\n",
    "    print(word,\":\",lemma.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging\n",
    "\n",
    "Input parameter must be tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Generating', 'VBG'), ('Values', 'NNS'), ('for', 'IN'), ('user', 'NN'), ('book', 'NN'), ('car', 'NN'), ('bike', 'NN'), ('and', 'CC'), ('resources', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "example_string = \"Generating Values for user, book, car, bike and resources\"\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', example_string)\n",
    "tokenized_words = word_tokenize(clean_text)\n",
    "tagged_string = pos_tag(tokenized_words)\n",
    "print(tagged_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking and Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "example_string = \"Generating Values for user, book, car, bike and resources\"\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', example_string)\n",
    "tokenized_words = word_tokenize(clean_text)\n",
    "tagged_string = pos_tag(tokenized_words)\n",
    "\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "tree = chunk_parser.parse(tagged_string)\n",
    "# tree.draw()\n",
    "\n",
    "chink_grammar = \"\"\"\n",
    "Chink: {<.*>+}\n",
    "       }<CC>{\"\"\"\n",
    "chink_parser = nltk.RegexpParser(chink_grammar)\n",
    "chink_tree = chink_parser.parse(tagged_string)\n",
    "chink_tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "* nltk.download('maxent_ne_chunker_tab')\n",
    "* nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "sample_string = \"Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded the company.\"\n",
    "tokenized_words = word_tokenize(sample_string)\n",
    "tagged_string = pos_tag(tokenized_words)\n",
    "tree = nltk.ne_chunk(tagged_string)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "* import nltk\n",
    "* nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "r , sir ?'--' Soon enough for any honest man that goes a passenger .' Ha ! Jon\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "\n",
    "corpus = gutenberg.words('melville-moby_dick.txt')\n",
    "text = Text(corpus)\n",
    "text.concordance([\"honest\", \"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding \n",
    "### Semantic Similarity Search\n",
    "\n",
    "pip install -U sentence-transformers\n",
    "\n",
    "Common Distance Metrics used\n",
    "* Euclidean\n",
    "* Manhattan\n",
    "* Minkowski\n",
    "* ChebyChev\n",
    "* Cosine Similarity\n",
    "* Hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: That is a happy person\n",
      "That is a very happy person  -> similarity score =  0.9429151\n",
      "That is a happy dog  -> similarity score =  0.6945774\n",
      "Today is a sunny day  -> similarity score =  0.25687614\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the model we want to use (it'll download itself)\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = [\n",
    "  \"That is a very happy person\",\n",
    "  \"That is a happy dog\",\n",
    "  \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "# vector embeddings created from dataset\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# query vector embedding\n",
    "query_embedding = model.encode(\"That is a happy person\")\n",
    "\n",
    "# define our distance metric\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "# run semantic similarity search\n",
    "print(\"Query: That is a happy person\")\n",
    "for e, s in zip(embeddings, sentences):\n",
    "    print(s, \" -> similarity score = \",\n",
    "         cosine_similarity(e, query_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PgVector With Python\n",
    "pip install psycopg2\n",
    "+ 384 is  for all-MiniLMV6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: That is a happy person\n",
      "Sentence: 3, Embedding: 0.3378904596229104\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(\"dbname=vector_db user=postgres password=1631\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS embeddings (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    vector_embeddings VECTOR(384)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def create_embedding():   \n",
    "    sentences = [\n",
    "    \"That is a happy dog\",\n",
    "    \"Today is a sunny day\",\n",
    "    \"That is a very happy person\"\n",
    "    ]\n",
    "    embeddings = model.encode(sentences)\n",
    "    for embedding in embeddings.tolist():\n",
    "        cursor.execute(\"INSERT INTO embeddings (vector_embeddings) VALUES (%s)\", (embedding,)) # Note ',' all embedding will not add\n",
    "    conn.commit()\n",
    "\n",
    "def query_embedding():\n",
    "    # Query sentence\n",
    "    query_sentence = \"That is a happy person\"\n",
    "    query_embedding = model.encode([query_sentence])[0].tolist()  # Convert numpy array to list\n",
    "\n",
    "    # Perform similarity search with pgvector\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT id, vector_embeddings <-> %s::vector AS similarity\n",
    "        FROM embeddings\n",
    "        ORDER BY similarity\n",
    "        LIMIT 2;\n",
    "    \"\"\", (query_embedding,))\n",
    "\n",
    "    # Retrieve and print results\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"Query: {query_sentence}\")\n",
    "    for row in results:\n",
    "        print(f\"Sentence: {row[0]}, Embedding: {row[1]}\")\n",
    "\n",
    "# create_embedding()\n",
    "query_embedding()\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
